{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing endless packages\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "import requests\n",
    "import random\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------Our Parameters-------------------#\n",
    "#add any categories you need to this list. This is the part of the url after \"/search\" on craigslist\n",
    "categories = [\"edu\", \"fbh\"]\n",
    "locations = [\"30033\",\"38139\",\"33101\",\"02802\",\"53558\",\"24501\",\"47906\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#headers to be used later\n",
    "headers_list = [\n",
    "    # Firefox 77 Mac\n",
    "    {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:77.0) Gecko/20100101 Firefox/77.0\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "    \"Referer\": \"https://www.google.com/\",\n",
    "    \"DNT\": \"1\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"Upgrade-Insecure-Requests\": \"1\"\n",
    "    },\n",
    "    # Chrome 92.0 Win10\n",
    "    {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "    \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "    \"Referer\": \"https://www.google.com/\",\n",
    "    \"DNT\": \"1\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"Upgrade-Insecure-Requests\": \"1\"\n",
    "    },\n",
    "    # Chrome 91.0 Win10\n",
    "    {\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"DNT\": \"1\",\n",
    "    \"Upgrade-Insecure-Requests\": \"1\",\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "    \"Sec-Fetch-Site\": \"none\",\n",
    "    \"Sec-Fetch-Mode\": \"navigate\",\n",
    "    \"Sec-Fetch-Dest\": \"document\",\n",
    "    \"Referer\": \"https://www.google.com/\",\n",
    "    \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "    \"Accept-Language\": \"en-GB,en-US;q=0.9,en;q=0.8\"\n",
    "    },\n",
    "    # Firefox 90.0 Win10\n",
    "    {\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"Upgrade-Insecure-Requests\": \"1\",\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:90.0) Gecko/20100101 Firefox/90.0\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "    \"Sec-Fetch-Site\": \"same-origin\",\n",
    "    \"Sec-Fetch-Mode\": \"navigate\",\n",
    "    \"Sec-Fetch-User\": \"?1\",\n",
    "    \"Sec-Fetch-Dest\": \"document\",\n",
    "    \"Referer\": \"https://www.google.com/\",\n",
    "    \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLinks(url):\n",
    "    chrome_options = webdriver.chrome.options.Options()\n",
    "    chrome_options.add_argument('--no-sandbox')# operate at the highest authority\n",
    "    chrome_options.add_argument('--disable-dev-shm-usage')#increase the RAM of chrome to load the page\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options = chrome_options)\n",
    "    #look within 1000 miles of Purdue. Doing this instead of individually searching multiple cities because Craigslist pages \n",
    "    #have different html tags by location, making finding links much more difficult\n",
    "    driver.get(url)\n",
    "    links = []\n",
    "    try:\n",
    "        while True:\n",
    "            #let the page load for 3 seconds\n",
    "            time.sleep(3)\n",
    "            html = driver.page_source\n",
    "            soup = bs(html, 'html.parser')\n",
    "            linkheaders = soup.find_all(\"a\", class_=\"result-title hdrlnk\")\n",
    "            for html_class in linkheaders:\n",
    "                #grab our quote, author, and tags\n",
    "                links.append(html_class.get(\"href\"))\n",
    "                    #try to find and click on a next button and loop\n",
    "            driver.find_element(By.PARTIAL_LINK_TEXT, \"next\").click()\n",
    "    #when we can no longer click on the next button, stop searching and quit out of the browser\n",
    "    except NoSuchElementException:\n",
    "        driver.quit() \n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to pull HTML text for each individual posting\n",
    "def getHTML(url):\n",
    "    #grab a random header to convince craiglist we aren't a bot\n",
    "    headers = random.choice(headers_list)\n",
    "    r = requests.Session()\n",
    "    r.headers = headers\n",
    "    #pull html from website and return it\n",
    "    htmltext = r.get(url).text\n",
    "    return htmltext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to pull out all of the titles and posting text blocks from Craiglist job postings\n",
    "def scrapeLinks(links):\n",
    "    titles = []\n",
    "    postings = []\n",
    "    numfailures = 0\n",
    "    for x in links:\n",
    "        time.sleep(.5)\n",
    "        if numfailures > 5:\n",
    "            break\n",
    "        html = getHTML(x)\n",
    "        attempts = 1\n",
    "        #Getting around Craigslist putting us in jail\n",
    "        #If they block our request, wait 1 second and try again. Do this up to 10 times\n",
    "        while html.find(\"Your request has been blocked\") >= 1 and attempts < 3:\n",
    "            attempts += 1\n",
    "            print(\"Craiglist blocked \", x, \"; making attempt #\",attempts, sep=\"\")\n",
    "            time.sleep(1)\n",
    "            html = getHTML(x)\n",
    "        if attempts < 10:  \n",
    "            try:\n",
    "                #Use beautiful soup to extract the post text block and title of the job\n",
    "                soup = bs(html, 'html.parser')\n",
    "                postsection = soup.find(\"section\", id=\"postingbody\")\n",
    "                posttitle = soup.find(\"span\", id=\"titletextonly\")\n",
    "                titles.append(posttitle.string)\n",
    "                #If we don't recognize this posting as a dup, add it to the list, otherwise, mark it as a dup\n",
    "                if postsection.text not in postings:\n",
    "                    print(posttitle.string)\n",
    "                    postings.append(postsection.text)\n",
    "                else:\n",
    "                    print(\"Duplicate!!!:\",posttitle.string)\n",
    "                    postings.append(\"Duplicate\")\n",
    "            except AttributeError: \n",
    "                print(html)\n",
    "        #if we couldn't pull this single post because Craigslist is mean, note our failure and proceed to the next posting      \n",
    "        else:\n",
    "            print(\"Unable to pull \",x)\n",
    "            postings.append(\"Failed\")\n",
    "            titles.append(\"Failed\")\n",
    "            numfailures +=1\n",
    "    #return our list of titles and post texx\n",
    "    return titles, postings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open up the csv that will store our resutls and write headers\n",
    "Output_File = open(\"JobPostingsEdu2.csv\", 'w',newline='',encoding=\"utf-8\")\n",
    "writer = csv.writer(Output_File)\n",
    "writer.writerow([\"Link\",\"Posting Title\", \"Location\", \"Category\", \"Posting Text\"])\n",
    "\n",
    "#loop through all requested categories\n",
    "for cat in categories:\n",
    "    for zip in locations:\n",
    "        searchurl = 'https://tippecanoe.craigslist.org/search/' + cat + \"?search_distance=250&postal=\" + zip\n",
    "        print(searchurl)\n",
    "        individuallinks = getLinks(searchurl)\n",
    "        #scrape the titles and posting details for all of the links we just stored\n",
    "        jobtitles, jobpostings = scrapeLinks(individuallinks)\n",
    "        #Barring some error/duplication of job posting, add it as a row to our csv\n",
    "        for x in range(len(jobpostings)):\n",
    "            if jobpostings[x] not in [\"Duplicate\",\"Failed\"]:\n",
    "                writer.writerow([individuallinks[x],jobtitles[x], individuallinks[x][8:individuallinks[x].find(\".\")], cat, jobpostings[x].strip().replace(\"\\n\",\" \")])\n",
    "#clean up and close our excel file when we are done\n",
    "Output_File.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "16eafed8952314ab23b1d41627a1738af2b91575998e694c63fcec6f213dd859"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
